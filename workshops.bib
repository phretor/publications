@InProceedings{   unruh_joernphp_2017,
  shorttitle    = {JoernPHP},
  author        = {Unruh, Tommi and Shastry, Bhargava and Skoruppa, Malte and
                  Maggi, Federico and Rieck, Konrad and Seifert, Jean-Pierre
                  and Yamaguchi, Fabian},
  title         = {Leveraging Flawed Tutorials for Seeding Large-Scale Web
                  Vulnerability Discovery},
  publisher     = {USENIX Association},
  booktitle     = {Proceedings of the 11th USENIX Workshop on Offensive
                  Technologies (WOOT 17)},
  location      = {Vancouver, BC},
  abstract      = {The Web is replete with tutorial-style content on how to
                  accomplish programming tasks. Unfortunately, even top-ranked
                  tutorials suffer from severe security vulnerabilities, such
                  as cross-site scripting (XSS), and SQL injection (SQLi).
                  Assuming that these tutorials influence real-world software
                  development, we hypothesize that code snippets from popular
                  tutorials can be used to bootstrap vulnerability discovery at
                  scale. To validate our hypothesis, we propose a
                  semi-automated approach to find recurring vulnerabilities
                  starting from a handful of top-ranked tutorials that contain
                  vulnerable code snippets. We evaluate our approach by
                  performing an analysis of tens of thousands of open-source
                  web applications to check if vulnerabilities originating in
                  the selected tutorials recur. Our analysis framework has been
                  running on a standard PC, analyzed 64,415 PHP codebases
                  hosted on GitHub thus far, and found a total of 117
                  vulnerabilities that have a strong syntactic similarity to
                  vulnerable code snippets present in popular tutorials. In
                  addition to shedding light on the anecdotal belief that
                  programmers reuse web tutorial code in an ad hoc manner, our
                  study finds disconcerting evidence of insufficiently reviewed
                  tutorials compromising the security of open-source projects.
                  Moreover, our findings testify to the feasibility of
                  large-scale vulnerability discovery using poorly written
                  tutorials as a starting point.},
  date          = {2017-08},
  url           = {https://www.usenix.org/conference/woot17/workshop-program/presentation/unruh},
  keywords      = {workshop},
  file          = {files/papers/workshop-papers/unruh_joernphp_2017.pdf}
}

@InProceedings{   shastry_hybridstaticfuzzing_2017,
  shorttitle    = {HybridStaticFuzzing},
  author        = {Shastry, Bhargava and Maggi, Federico and Yamaguchi, Fabian
                  and Rieck, Konrad and Seifert, Jean-Pierre},
  title         = {Static Exploration of Taint-Style Vulnerabilities Found by
                  Fuzzing},
  publisher     = {USENIX Association},
  booktitle     = {11th USENIX Workshop on Offensive Technologies USENIX
                  Workshop on Offensive Technologies ({WOOT} 17)},
  location      = {Vancouver, BC},
  abstract      = {Taint-style vulnerabilities comprise a majority of fuzzer
                  discovered program faults. These vulnerabilities usually
                  manifest as memory access violations caused by tainted
                  program input. Although fuzzers have helped uncover a
                  majority of taint-style vulnerabilities in software to date,
                  they are limited by (i) extent of test coverage; and (ii) the
                  availability of fuzzable test cases. Therefore, fuzzing alone
                  cannot provide a high assurance that all taint-style
                  vulnerabilities have been uncovered.
                  
                  In this paper, we use static template matching to find
                  recurrences of fuzzer-discovered vulnerabilities. To
                  compensate for the inherent incompleteness of template
                  matching, we implement a simple yet effective match-ranking
                  algorithm that uses test coverage data to focus attention on
                  matches comprising untested code. We prototype our approach
                  using the Clang/LLVM compiler toolchain and use it in
                  conjunction with afl-fuzz, a modern coverage-guided fuzzer.
                  Using a case study carried out on the Open vSwitch codebase,
                  we show that our prototype uncovers corner cases in modules
                  that lack a fuzzable test harness. Our work demonstrates that
                  static analysis can effectively complement fuzz testing, and
                  is a useful addition to the security assessment tool-set.
                  Furthermore, our techniques hold promise for increasing the
                  effectiveness of program analysis and testing, and serve as a
                  building block for a hybrid vulnerability discovery
                  framework.},
  date          = {2017-08},
  url           = {https://www.usenix.org/conference/woot17/workshop-program/presentation/shastry},
  keywords      = {workshop},
  file          = {files/papers/workshop-papers/shastry_hybridstaticfuzzing_2017.pdf}
}

@InProceedings{   polakis_osnresearch_2014,
  shorttitle    = {OSNResearch},
  author        = {Polakis, Iasonas and Maggi, Federico and Zanero, Stefano and
                  Keromytis, Angelos D.},
  title         = {Security and Privacy Measurements on Social Networks:
                  Experiences and Lessons Learned},
  booktitle     = {2014 Third International Workshop on Building Analysis
                  Datasets and Gathering Experience Returns for Security
                  (BADGERS)},
  pages         = {18-29},
  location      = {Wroclaw, Poland},
  abstract      = {We describe our experience gained while exploring practical
                  security and privacy problems in a real-world, large- scale
                  social network (i.e., Facebook), and summarize our
                  conclusions in a series of "lessons learned". We first
                  conclude that it is better to adequately describe the
                  potential ethical concerns from the very beginning and plan
                  ahead the institutional review board (IRB) request. Even
                  though sometimes optional, the IRB approval is a valuable
                  point from the reviewer's perspective. Another aspect that
                  needs planning is getting in touch with the online social
                  network security team, which takes a substantial amount of
                  time. With their support, "bending the rules" (e.g., using
                  scrapers) when the experimental goals require so, is easier.
                  Clearly, in cases where critical technical vulnerabilities
                  are found during the research, the general recommendations
                  for responsible disclosure should be followed. Gaining the
                  audience's engagement and trust was essential to the success
                  of our user study. Participants felt more comfortable when
                  subscribing to our experiments, and also responsibly reported
                  bugs and glitches. We did not observe the same behavior in
                  crowd-sourcing workers, who were instead more interested in
                  obtaining their rewards. On a related point, our experience
                  suggests that crowd sourcing should not be used alone:
                  Setting up tasks is more time consuming than it seems, and
                  researchers must insert some sentinel checks to ensure that
                  workers are not submitting random answers.From a logistics
                  point of view, we learned that having at least a high-level
                  plan of the experiments pays back, especially when the IRB
                  requires a detailed description of the work and the data to
                  be collected. However, over planning can be dangerous because
                  the measurement goals can change dynamically. From a
                  technical point of view, partially connected to the logistics
                  remarks, having a complex and large data-gathering and
                  analysis framework may be counterproductive in terms of
                  set-up a- d management overhead. From our experience we
                  suggest to choose simple technologies that scale up if needed
                  but, more importantly, can scale down. For example, launching
                  a quick query should be straightforward, and the frameworks
                  should not impose too much overhead for formulating it. We
                  conclude with a series of practical recommendations on how to
                  successfully collect data from online social networks (e.g.,
                  using techniques for network multipresence, mimicking user
                  behavior, and other crawling "tricks"') and avoid abusing the
                  online service, while gathering the data required by the
                  experiments.},
  doi           = {10.1109/BADGERS.2014.9},
  date          = {2014-09},
  keywords      = {workshop},
  file          = {files/papers/workshop-papers/polakis_osnresearch_2014.pdf}
}

@InProceedings{   maggi_andrototal_2013,
  shorttitle    = {AndroTotal},
  author        = {Maggi, Federico and Valdi, Andrea and Zanero, Stefano},
  title         = {AndroTotal: A Flexible, Scalable Toolbox and Service for
                  Testing Mobile Malware Detectors},
  publisher     = {ACM},
  booktitle     = {Proceedings of the Third ACM Workshop on Security and
                  Privacy in Smartphones \& Mobile Devices},
  series        = {SPSM '13},
  pages         = {49--54},
  location      = {New York, NY, USA},
  abstract      = {Although there are controversial opinions regarding how
                  large the mobile malware phenomenon is in terms of absolute
                  numbers, hype aside, the amount of new Android malware
                  variants is increasing. This trend is mainly due to the fact
                  that, as it happened with traditional malware, the authors
                  are striving to repackage, obfuscate, or otherwise transform
                  the executable code of their malicious apps in order to evade
                  mobile security apps. There are about 85 of these apps only
                  on the official marketplace. However, it is not clear how
                  effective they are. Indeed, the sandboxing mechanism of
                  Android does not allow (security) apps to audit other apps.
                  We present AndroTotal, a publicly available tool, malware
                  repository and research framework that aims at mitigating the
                  above challenges, and allow researchers to automatically scan
                  Android apps against an arbitrary set of malware detectors.
                  We implemented AndroTotal and released it to the research
                  community in April 2013. So far, we collected 18,758 distinct
                  submitted samples and received the attention of several
                  research groups (1,000 distinct accounts), who integrated
                  their malware-analysis services with ours.},
  doi           = {10.1145/2516760.2516768},
  isbn          = {978-1-4503-2491-5},
  date          = {2013-10},
  url           = {http://doi.acm.org/10.1145/2516760.2516768},
  keywords      = {workshop},
  file          = {files/papers/workshop-papers/maggi_andrototal_2013.pdf}
}

@InProceedings{   maggi_syssecpolimi_2011,
  shorttitle    = {SysSecPOLIMI},
  author        = {Maggi, Federico and Zanero, Stefano},
  title         = {System Security research at Politecnico di Milano},
  publisher     = {IEEE Computer Society},
  booktitle     = {Proceedings of the 1st SysSec Workshop (SysSec)},
  abstract      = {This paper summarizes the past, present and future lines of
                  research in the systems security area pursued by the
                  Performance Evaluation Lab of Politecnico di Milano. We
                  describe our past research in the area of learning algorithms
                  applied to intrusion detection, our current work in the area
                  of malware analysis, and our future research outlook,
                  oriented to the cloud, to mobile device security, and to
                  cyber-physical systems.},
  doi           = {10.1109/SysSec.2011.30},
  date          = {2011-07-06},
  keywords      = {workshop},
  file          = {files/papers/workshop-papers/maggi_syssecpolimi_2011.pdf}
}

@InProceedings{   maggi_phonephishinghoneypot_2011,
  shorttitle    = {PhonePhishingHoneypot},
  author        = {Maggi, Federico and Sisto, Alessandro and Zanero, Stefano},
  title         = {A social-engineering-centric data collection initiative to
                  study phishing},
  publisher     = {ACM},
  booktitle     = {Proceedings of the First Workshop on Building Analysis
                  Datasets and Gathering Experience Returns for Security
                  (BADGERS)},
  pages         = {107--108},
  location      = {New York, NY, USA},
  abstract      = {Phishers nowadays rely on a variety of channels, ranging
                  from old-fashioned emails to instant messages, social
                  networks, and the phone system (with both calls and text
                  messages), with the goal of reaching more victims. As a
                  consequence, modern phishing became a multi-faceted, even
                  more pervasive threat that is inherently more difficult to
                  study than traditional, email-based phishing. This short
                  paper describes the status of a data collection system we are
                  developing to capture different aspects of phishing
                  campaigns, with a particular focus on the emerging use of the
                  voice channel. The general approach is to record inbound
                  calls received on decoy phone lines, place outbound calls to
                  the same caller identifiers (when available) and also to
                  telephone numbers obtained from different sources.
                  Specifically, our system analyzes instant messages (e.g.,
                  automated social engineering attempts) and suspicious emails
                  (e.g., spam, phishing), and extracts telephone numbers, URLs
                  and popular words from the content. In addition, users can
                  voluntarily submit voice phishing (vishing) attempts through
                  a public website. Extracted telephone numbers, URLs and
                  popular words will be correlated to recognize campaigns by
                  means of cross-channel relationships between messages.},
  doi           = {10.1145/1978672.1978687},
  isbn          = {978-1-4503-0768-0},
  date          = {2011-04-10},
  keywords      = {workshop},
  file          = {files/papers/workshop-papers/maggi_phonephishinghoneypot_2011.pdf}
}
